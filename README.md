# AI CLI - Low-Level Multi-Agent Development System

–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π Python –∫–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Ollama –∏ –¥—Ä—É–≥–∏–º–∏ –æ—Ñ—Ñ–ª–∞–π–Ω –º–æ–¥–µ–ª—è–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –¥–ª—è collaborative —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.

## –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π HTTP –∫–ª–∏–µ–Ω—Ç** –¥–ª—è Ollama —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–¥ –∑–∞–ø—Ä–æ—Å–∞–º–∏
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏ –ü–ö** - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ CPU, RAM, GPU
- **–ö–æ–Ω—Ç—Ä–æ–ª—å system prompts** (`application/vnd.ollama.image.system`) –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–æ–≤
- **–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞** –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –ø—Ä–∏ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏ –∫–æ–¥–∞
- **–ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤** - –ª–µ–≥–∫–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –æ—Ñ—Ñ–ª–∞–π–Ω –º–æ–¥–µ–ª–µ–π (llama.cpp, GGUF –∏ –¥—Ä.)
- **Async/await** –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã
- **HTTP/2** –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

## –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
ai_cli/
‚îú‚îÄ‚îÄ client.py              # –ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π Ollama HTTP –∫–ª–∏–µ–Ω—Ç
‚îú‚îÄ‚îÄ agent.py               # –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞
‚îú‚îÄ‚îÄ prompt_manager.py      # –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ system prompts
‚îú‚îÄ‚îÄ models.py              # –ê–±—Å—Ç—Ä–∞–∫—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π
‚îú‚îÄ‚îÄ resource_monitor.py    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ —Å–∏—Å—Ç–µ–º—ã
‚îú‚îÄ‚îÄ types.py               # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
‚îî‚îÄ‚îÄ cli.py                 # CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å
```

## üìã –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

- üöÄ **[QUICKSTART.md](QUICKSTART.md)** - –ù–∞—á–Ω–∏—Ç–µ —Ä–∞–±–æ—Ç—É –∑–∞ 5 –º–∏–Ω—É—Ç
- üì¶ **[INSTALL.md](INSTALL.md)** - –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö
- üîß **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** - –†–µ—à–µ–Ω–∏–µ —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º
- üí° **–ü—Ä–∏–º–µ—Ä—ã** - —Å–º. –ø–∞–ø–∫—É `examples/`
- ü©∫ **`python diagnose.py`** - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã

## –°–∏—Å—Ç–µ–º–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

**–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Python: 3.11 –∏–ª–∏ 3.12**

‚ö†Ô∏è **–í–ê–ñ–ù–û**: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ Python 3.13+ - –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –µ—â–µ –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º—ã.

–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–∞—à—É –≤–µ—Ä—Å–∏—é Python:
```bash
python --version
```
```bash
# –∏–ª–∏
python3 --version
```

–ï—Å–ª–∏ —É –≤–∞—Å Python 3.13+, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Python 3.12:
- **macOS**: `brew install python@3.12`
- **Windows**: –°–∫–∞—á–∞–π—Ç–µ —Å [python.org](https://www.python.org/downloads/)
- **Linux**: `sudo apt install python3.12` –∏–ª–∏ `sudo yum install python3.12`

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –®–∞–≥ 1: –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω

```bash
# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —á—Ç–æ Ollama —Ä–∞–±–æ—Ç–∞–µ—Ç
curl http://localhost:11434/api/tags

# –ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama:
# macOS/Linux: https://ollama.ai/download
# –ò–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ: ollama serve
```

### –®–∞–≥ 2: –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ

**–ß—Ç–æ —Ç–∞–∫–æ–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ?**
–≠—Ç–æ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è Python –ø–∞–∫–µ—Ç–æ–≤ –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤ —Å –¥—Ä—É–≥–∏–º–∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏.

```bash
# –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞
cd /path/to/ai-cli
```
```bash
# –°–æ–∑–¥–∞–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ (–¥–µ–ª–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑)
python3.12 -m venv venv
```
```bash
# –∏–ª–∏ –µ—Å–ª–∏ python3.12 –Ω–µ –Ω–∞–π–¥–µ–Ω:
python3 -m venv venv
```
* –ê–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
* –ù–∞ macOS/Linux:
```bash
source venv/bin/activate
```
```bash
# –ù–∞ Windows:
venv\Scripts\activate

# –ü–æ—Å–ª–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤—ã —É–≤–∏–¥–∏—Ç–µ (venv) –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏ —Ç–µ—Ä–º–∏–Ω–∞–ª–∞
```

### –®–∞–≥ 3: –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏

```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ (–≤–∏–¥–∏—Ç–µ (venv) –≤ —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)

# –û–±–Ω–æ–≤–∏—Ç–µ pip –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏
pip3 install --upgrade pip
```
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip3 install -r requirements.txt
```
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ ai_cli –≤ —Ä–µ–∂–∏–º–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
pip3 install -e .
```
```bash
# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–ª—è llama.cpp –ø–æ–¥–¥–µ—Ä–∂–∫–∏ (GGUF –º–æ–¥–µ–ª–∏)
pip3 install llama-cpp-python
```
```bash
# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ NVIDIA GPU
pip3 install pynvml
```

### –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É

```bash
# –î–æ–ª–∂–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –æ—à–∏–±–æ–∫
python -c "import ai_cli; print('‚úì AI CLI installed successfully')"
```
```bash
# –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
python examples/basic_usage.py
```

**–ü—Ä–æ–±–ª–µ–º—ã —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π?** –°–º. **[INSTALL.md](INSTALL.md)** –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏–ª–∏ **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)** –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º.

### –ö–∞–∫ –≤—ã–∫–ª—é—á–∏—Ç—å –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ

```bash
# –ö–æ–≥–¥–∞ –∑–∞–∫–æ–Ω—á–∏—Ç–µ —Ä–∞–±–æ—Ç—É
deactivate
```

### –ö–∞–∫ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Å–Ω–æ–≤–∞

```bash
# –í —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑ –ø—Ä–æ—Å—Ç–æ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∑–∞–Ω–æ–≤–æ
cd /path/to/ai-cli
source venv/bin/activate  # macOS/Linux
```
```bash
# –∏–ª–∏
venv\Scripts\activate     # Windows
```

## –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –≤ —Ñ–∞–π–ª—ã

```python
import asyncio
from pathlib import Path
from ai_cli import OllamaClient
from ai_cli.agent import AgentOrchestrator

async def main():
    async with OllamaClient() as client:
        # –°–æ–∑–¥–∞–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–¥–∞
        orch = AgentOrchestrator(
            client,
            output_dir=Path("./my_project"),
            auto_save_code=True  # –ö–æ–¥ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ —Ñ–∞–π–ª—ã!
        )

        team = orch.create_coding_team(specializations=["backend"])

        # –ê–≥–µ–Ω—Ç –Ω–∞–ø–∏—à–µ—Ç –∫–æ–¥ –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç –≤ —Ñ–∞–π–ª—ã
        results = await orch.distribute_task(
            "–°–æ–∑–¥–∞–π FastAPI endpoint –¥–ª—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
        )

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –±—ã–ª–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        for result in results:
            if "saved_files" in result:
                print("–°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:", result["saved_files"])

asyncio.run(main())
```

### 2. –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

```python
import asyncio
from ai_cli import OllamaClient

async def main():
    async with OllamaClient() as client:
        response = await client.generate(
            model="llama3.2",
            prompt="Write a Python function to calculate fibonacci",
            system="You are a helpful coding assistant"
        )
        print(response.response)

asyncio.run(main())
```

### 2. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞

```python
import asyncio
from ai_cli import OllamaClient
from ai_cli.agent import AgentOrchestrator

async def main():
    async with OllamaClient() as client:
        # –°–æ–∑–¥–∞–µ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä
        orchestrator = AgentOrchestrator(client)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–∞–Ω–¥—É —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∞–≥–µ–Ω—Ç–æ–≤
        team = orchestrator.create_coding_team(
            model="llama3.2",
            specializations=["backend", "frontend", "testing", "review"]
        )

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–¥–∞—á—É –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏
        task = "Design a REST API for user authentication"
        results = await orchestrator.distribute_task(task)

        # –ö–∞–∂–¥—ã–π –∞–≥–µ–Ω—Ç –¥–∞—Å—Ç —Å–≤–æ–µ –≤–∏–¥–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
        for result in results:
            print(f"{result['agent']}: {result['response'][:200]}...")

asyncio.run(main())
```

### 3. Workflow —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏

```python
# –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π workflow
workflow = [
    {
        "agent": "backend_agent",
        "task": "Write a user registration endpoint"
    },
    {
        "agent": "testing_agent",
        "task": "Write tests for the registration endpoint",
        "use_context": True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∞–≥–µ–Ω—Ç–∞
    },
    {
        "agent": "review_agent",
        "task": "Review code and suggest improvements",
        "use_context": True
    }
]

results = await orchestrator.coordinate_workflow(workflow)
```

### 4. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ System Prompts

```python
from ai_cli.prompt_manager import PromptManager

# –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–º–ø—Ç–æ–≤
pm = PromptManager()

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π system prompt
pm.register_prompt(
    "security_expert",
    "You are a cybersecurity expert specializing in code security audits."
)

# –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞ —Å —ç—Ç–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
config = pm.create_agent_config(
    name="security_agent",
    role="Security Auditor",
    prompt_template=pm.get_prompt("security_expert"),
    model="llama3.2",
    temperature=0.3
)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥–µ–Ω—Ç–∞
agent = orchestrator.register_agent(config)
result = await agent.execute_task("Review this code for vulnerabilities: ...")
```

### 5. –†–∞–±–æ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏

```python
from ai_cli.models import create_standard_registry

# –°–æ–∑–¥–∞–µ–º registry —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
registry = await create_standard_registry(
    ollama_url="http://localhost:11434",
    llama_cpp_model="/path/to/model.gguf"  # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–∫—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
provider = registry.get_active_provider()
response = await provider.generate(
    prompt="Explain Python decorators",
    model="llama3.2"
)

# –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ –¥—Ä—É–≥–æ–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
registry.set_active_provider("llama_cpp")
provider = registry.get_active_provider()
```

### 6. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤

```python
async with OllamaClient(enable_resource_monitoring=True) as client:
    # –ö–ª–∏–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç —Ä–µ—Å—É—Ä—Å—ã
    response = await client.generate(
        model="llama3.2",
        prompt="Complex task..."
    )

    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
    usage = client.get_resource_usage()
    print(f"CPU: {usage['cpu_percent']}%")
    print(f"Memory: {usage['memory_percent']}%")
    print(f"Available RAM: {usage['memory_available_gb']} GB")
```

## CLI –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å

```bash
# –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
python -m ai_cli.cli chat

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—É—é –∫–æ–º–∞–Ω–¥—É
python -m ai_cli.cli team "Create a user authentication system"

# –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
python -m ai_cli.cli models
```

## –ü—Ä–∏–º–µ—Ä—ã

–°–º–æ—Ç—Ä–∏—Ç–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é `examples/`:
- `basic_usage.py` - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã —Ä–∞–±–æ—Ç—ã —Å –∫–ª–∏–µ–Ω—Ç–æ–º
- `multi_agent.py` - –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã
- `model_providers.py` - —Ä–∞–±–æ—Ç–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏ –º–æ–¥–µ–ª–µ–π

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–æ–≤

### –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞

```python
from ai_cli.types import AgentConfig

config = AgentConfig(
    name="rust_expert",
    role="Rust Developer",
    system_prompt="""You are an expert Rust developer.
    Focus on memory safety, performance, and idiomatic Rust code.
    Always explain ownership and borrowing when relevant.""",
    model="llama3.2",
    temperature=0.7,
    max_tokens=2000
)

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–∞
agent = orchestrator.register_agent(config)
```

### –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Modelfile

```python
# AgentConfig –º–æ–∂–µ—Ç —Å–æ–∑–¥–∞—Ç—å Modelfile –¥–ª—è Ollama
modelfile = config.to_modelfile()
print(modelfile)
# –í—ã–≤–µ–¥–µ—Ç:
# FROM llama3.2
# SYSTEM You are an expert Rust developer...
# PARAMETER temperature 0.7
# PARAMETER num_predict 2000

# –°–æ–∑–¥–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å –≤ Ollama
await client.create_model(
    name="rust-expert",
    modelfile=modelfile
)
```

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

### Streaming –æ—Ç–≤–µ—Ç–æ–≤

```python
# –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
async for chunk in await client.generate(
    model="llama3.2",
    prompt="Explain async programming",
    stream=True
):
    print(chunk.response, end="", flush=True)
```

### –ö–æ–Ω—Ç—Ä–æ–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

```python
# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
response1 = await client.generate(
    model="llama3.2",
    prompt="Start a story about a programmer"
)

# –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–º –∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
response2 = await client.generate(
    model="llama3.2",
    prompt="Continue the story",
    context=response1.context  # –ü–µ—Ä–µ–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
)
```

### –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤

```python
from ai_cli.resource_monitor import ResourceMonitor

monitor = ResourceMonitor(check_interval=0.5)
await monitor.start()

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –∑–∞–ø—Ä–æ—Å—ã
if monitor.should_throttle(cpu_threshold=80, memory_threshold=85):
    print("System under load, throttling requests...")
    await asyncio.sleep(1)
```

## –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π

### llama.cpp

```python
from ai_cli.models import LlamaCppProvider

provider = LlamaCppProvider(model_path="/path/to/model.gguf")
await provider.initialize(
    n_ctx=4096,
    n_gpu_layers=-1  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GPU
)

response = await provider.generate(
    prompt="Hello!",
    system="You are helpful"
)
```

### –ö–∞—Å—Ç–æ–º–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä

```python
from ai_cli.models import ModelProvider, ModelType

class CustomProvider(ModelProvider):
    def __init__(self):
        super().__init__(ModelType.CUSTOM)

    async def initialize(self, **kwargs):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
        pass

    async def generate(self, prompt, system=None, **kwargs):
        # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        pass

    async def chat(self, messages, **kwargs):
        # –í–∞—à–∞ –ª–æ–≥–∏–∫–∞ —á–∞—Ç–∞
        pass

# –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –≤ registry
registry.register_provider("my_model", CustomProvider())
```

## –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

1. **–ù–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å**: –ü—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø –∫ HTTP API Ollama
2. **–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å**: HTTP/2, connection pooling, async/await
3. **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ CPU, RAM, GPU –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
4. **–ì–∏–±–∫–æ—Å—Ç—å**: –õ–µ–≥–∫–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å –ª—é–±—É—é –æ—Ñ—Ñ–ª–∞–π–Ω –º–æ–¥–µ–ª—å
5. **–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø—Ä–æ–µ–∫—Ç–æ–≤
6. **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞–º–∏**: –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å system prompts

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- **Python 3.11 –∏–ª–∏ 3.12** (—Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–æ, –ù–ï 3.13+)
- **Ollama** –∑–∞–ø—É—â–µ–Ω–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω–æ (`http://localhost:11434`) –∏–ª–∏ –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω–æ–º —Å–µ—Ä–≤–µ—Ä–µ
- –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: httpx, pydantic, psutil (—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏)

## –õ–∏—Ü–µ–Ω–∑–∏—è

MIT

## –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
pip install -e .

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
python examples/basic_usage.py
python examples/multi_agent.py
python examples/model_providers.py
```

## Roadmap

- [ ] –ü–æ–¥–¥–µ—Ä–∂–∫–∞ Hugging Face Transformers
- [ ] –í–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞–º–∏
- [ ] –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤
- [ ] –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å VS Code
- [ ] Distributed multi-agent systems (—Ä–∞–±–æ—Ç–∞ —á–µ—Ä–µ–∑ —Å–µ—Ç—å)
- [ ] Benchmark —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
- [ ] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–¥–∞ –∞–≥–µ–Ω—Ç–∞–º–∏
